{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new list to append urls\n",
    "post_urls = []\n",
    "url = \"https://www.whitehouse.gov\"\n",
    "\n",
    "# 50 pages of posts\n",
    "for i in np.arange(50):\n",
    "    #grab url and append page number\n",
    "    page_url = url + \"/blog?page=\" + str(i)\n",
    "    r = requests.get(page_url)\n",
    "    soup = bs4.BeautifulSoup(r.content.decode('utf-8'), \"lxml\")\n",
    "    # h3 field-content is the tag to get post urls\n",
    "    page_posts = soup.find_all(\"h3\", \"field-content\")\n",
    "    # dates for the posts\n",
    "    post_dates = soup.find_all(\"span\", \"field-content\")\n",
    "    # append page's post urls to our list\n",
    "    post_urls.append([(post.text, post.find('a')['href'], date.text) for post, date in zip(page_posts, post_dates)])\n",
    "\n",
    "# flatten list\n",
    "all_posts = [p for post in post_urls for p in post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posts = []\n",
    "\n",
    "# loop through every post and get text in post\n",
    "for post_data in all_posts:\n",
    "    req = requests.get(url + post_data[1], allow_redirects=False)\n",
    "    if req.status_code == 200:\n",
    "        req_soup = bs4.BeautifulSoup(req.text, \"lxml\")\n",
    "        post_body = req_soup.find(\"div\",\"pane-entity-field\").text\\\n",
    "        .encode('ascii', 'ignore').decode('UTF-8').replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        posts.append(post_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame({'a': [post[2].strip() for post in all_posts], 'b': [post[0] for post in all_posts], 'c': [post.strip() for post in posts]})\n",
    "df_out.columns = ['Date', 'Title', 'Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_out.to_csv('WH_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Everything above was used to create our CSV file\n",
    "# There is no need to run the cells above\n",
    "# You can start from the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('WH_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "attach-environment": true,
   "summary": "History 100S Jupyter Notebook",
   "url": "https://anaconda.org/jkhaykin/hist100final"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
